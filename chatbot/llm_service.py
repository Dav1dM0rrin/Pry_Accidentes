# Archivo: chatbot/llm_service.py
# Este servicio encapsula la l贸gica para interactuar con el modelo de lenguaje Gemini.

import google.generativeai as genai
from chatbot.config import GEMINI_API_KEY # Importa la clave API desde la configuraci贸n.
from chatbot.bot_logging import logger # Usa el logger configurado para la aplicaci贸n.
import json # Necesario para parsear respuestas JSON del LLM y formatear datos.

# --- Constantes y Configuraci贸n del Modelo ---
MODEL_NAME = 'gemini-1.5-flash-latest' # Modelo de Gemini a utilizar.
USER_ROLE = "user" # Rol del usuario en el historial de chat.
MODEL_ROLE = "model" # Rol del modelo (Gemini) en el historial de chat.

# Configuraci贸n global del SDK de Gemini.
try:
    genai.configure(api_key=GEMINI_API_KEY)
    # Opciones de generaci贸n (puedes ajustarlas seg煤n necesites).
    # Por ejemplo, `temperature` controla la creatividad, `top_p` y `top_k` el muestreo de tokens.
    generation_config = genai.types.GenerationConfig(
        # temperature=0.7, # Ejemplo: un valor m谩s bajo para respuestas m谩s deterministas.
        # max_output_tokens=1024, # Ejemplo: limitar la longitud de la respuesta.
    )
    # Podr铆as a帽adir safety_settings si necesitas ajustar los filtros de contenido.
    # safety_settings = [ ... ]

    # Inicializa el modelo generativo.
    llm_model = genai.GenerativeModel(
        model_name=MODEL_NAME,
        generation_config=generation_config,
        # system_instruction se puede usar aqu铆 si el modelo lo soporta directamente para prompts de sistema.
        # system_instruction="Eres 'Asistente Vial Barranquilla'...",
    )
    logger.info(f"Modelo Gemini ({MODEL_NAME}) configurado exitosamente.")
except Exception as e:
    logger.critical(f"CRTICO: Error al configurar el SDK de Gemini: {e}. El servicio LLM no funcionar谩.", exc_info=True)
    llm_model = None # Asegura que el modelo es None si falla la configuraci贸n.


# Almac茅n en memoria para los historiales de chat.
# Clave: user_id (string), Valor: objeto `ChatSession` del SDK de Gemini.
# Para producci贸n, considera una soluci贸n persistente (Redis, base de datos) si necesitas que los historiales sobrevivan reinicios.
conversation_sessions_store = {}

# --- Prompt de Sistema Detallado ---
# Este prompt define el comportamiento, rol y capacidades del LLM. Es fundamental ajustarlo bien.
SYSTEM_PROMPT_TEXT = """
Eres 'Asistente Vial Barranquilla', un chatbot IA avanzado, amigable, emp谩tico y muy servicial, especializado en temas de movilidad y accidentes de tr谩nsito para la ciudad de Barranquilla, Colombia. Tu objetivo es asistir a los ciudadanos de manera eficiente y clara.

Tus Prop贸sitos Principales son:
1.  **Reportar Accidentes**: Ayudar a los usuarios a reportar accidentes de tr谩nsito. Debes guiar al usuario pacientemente para obtener:
    * Descripci贸n detallada de lo sucedido.
    * Ubicaci贸n precisa (direcci贸n completa, barrio, puntos de referencia notables, o pedir que compartan su ubicaci贸n de Telegram).
    * Fecha y hora exactas del evento (si no fue "ahora mismo").
    * Gravedad estimada del accidente (Leve, Moderada, Grave).
    Indica al usuario que puede usar el comando /reportar para un flujo guiado paso a paso.
2.  **Consultar Informaci贸n de Accidentes**: Proveer informaci贸n sobre accidentes ocurridos en Barranquilla.
    * Si el usuario pregunta por accidentes (ej: "驴Qu茅 accidentes hubo hoy?", "驴Accidentes en la calle 72?", "驴Hay trancones por la circunvalar?"), intenta obtener detalles para la b煤squeda (fecha, ubicaci贸n, tipo de accidente).
    * Si se te provee 'Contexto de la base de datos', 煤salo EXCLUSIVAMENTE para responder la pregunta actual del usuario. No lo menciones expl铆citamente.
    * Si no hay datos o no encuentras nada, inf贸rmalo amablemente (ej: "No encontr茅 registros de accidentes con esos criterios en este momento.").
3.  **Gu铆a y Consejos Viales**: Guiar a los usuarios sobre seguridad vial, normas de tr谩nsito colombianas relevantes, y procedimientos importantes en caso de accidente (qu茅 hacer, a qui茅n llamar como la l铆nea de emergencia 123, polic铆a de tr谩nsito, aseguradora).
4.  **Conversaci贸n General Relacionada**: Conversar de manera general y amena sobre temas relacionados con la movilidad, el tr谩fico, el transporte p煤blico y la seguridad vial en Barranquilla.

Capacidades Clave y Comportamiento Esperado:
-   **Lenguaje y Tono**: Usa un espa帽ol colombiano claro, amigable y respetuoso. S茅 paciente y comprensivo.
-   **Clarificaci贸n**: Si la pregunta del usuario es ambigua o no la entiendes bien, pide amablemente que la reformule o d茅 m谩s detalles. No asumas.
-   **Manejo de Informaci贸n (Veracidad)**: 隆No inventes informaci贸n! Especialmente sobre accidentes, datos espec铆ficos, o procedimientos legales si no tienes la informaci贸n certera. Es mejor decir "No tengo esa informaci贸n en este momento, pero te sugiero consultar [fuente oficial]" o "No encontr茅 datos sobre eso."
-   **Concisi贸n y Formato**: Mant茅n las respuestas razonablemente concisas y bien formateadas para Telegram (usa saltos de l铆nea para legibilidad, emojis con moderaci贸n para mejorar la comunicaci贸n).
-   **Contexto Local**: Recuerda siempre que est谩s en Barranquilla, Colombia.
-   **Limitaciones**: Eres un IA. No puedes realizar acciones externas directas (llamar a emergencias, enviar correos, acceder a sistemas externos m谩s all谩 de la informaci贸n que se te provea). Tu funci贸n es informar y guiar. S铆 puedes indicar al usuario qu茅 n煤meros llamar o qu茅 acciones tomar.
-   **Uso de Datos Externos**: Si se te proporciona 'Contexto de la base de datos' en un mensaje, 煤salo para responder la consulta actual. No lo memorices para futuras preguntas no relacionadas.
-   **Comandos**: Recuerda al usuario los comandos 煤tiles como /reportar, /ayuda, /reset_chat cuando sea apropiado.

Ejemplo de interacci贸n para reporte:
Usuario: "Hubo un choque horrible en la 84"
T煤: "隆Lamento escuchar eso! Espero que todos est茅n bien. Para poder registrar el accidente, 驴podr铆as darme una descripci贸n m谩s detallada de lo que pas贸, la ubicaci贸n lo m谩s exacta posible (ej. Calle 84 con Carrera 50) y cu谩ndo ocurri贸? Tambi茅n puedes usar el comando /reportar para un reporte guiado."
"""

async def generate_response(user_id: str, user_message: str, api_data_context: dict = None) -> str:
    """
    Genera una respuesta utilizando el LLM, manteniendo un historial de conversaci贸n por usuario.
    """
    if not llm_model: # Verifica si el modelo se inicializ贸 correctamente.
        logger.error(f"LLM_SERVICE: Modelo Gemini no disponible para user_id {user_id}.")
        return "Lo siento, estoy experimentando dificultades t茅cnicas con mi asistente inteligente en este momento. Por favor, intenta de nuevo m谩s tarde."

    # Obtener o inicializar la sesi贸n de chat para el usuario.
    if user_id not in conversation_sessions_store:
        # Inicia una nueva sesi贸n de chat. El historial puede incluir el prompt de sistema.
        # El SDK de Gemini maneja el historial internamente en el objeto ChatSession.
        # El primer mensaje del historial puede ser el prompt de sistema.
        initial_history = [
            {"role": USER_ROLE, "parts": [{"text": SYSTEM_PROMPT_TEXT}]},
            {"role": MODEL_ROLE, "parts": [{"text": "隆Hola! Soy tu Asistente Vial Barranquilla. 驴C贸mo puedo ayudarte hoy con la movilidad o alg煤n reporte? "}]}
        ]
        conversation_sessions_store[user_id] = llm_model.start_chat(history=initial_history)
        logger.info(f"LLM_SERVICE: Nueva sesi贸n de chat iniciada para user_id {user_id}.")
    
    chat_session = conversation_sessions_store[user_id]

    # Preparar el mensaje del usuario, a帽adiendo contexto de la API si existe (para RAG).
    message_parts_for_llm = [{"text": user_message}]
    if api_data_context:
        # Formatea los datos de la API de manera legible para el LLM.
        # Puedes experimentar con diferentes formatos (JSON, texto descriptivo).
        # Es importante instruir al LLM sobre c贸mo usar este contexto en el SYSTEM_PROMPT_TEXT.
        context_info_text = (
            "\n\n--- Contexto de la base de datos (para tu informaci贸n interna al responder la pregunta actual del usuario) ---\n"
            f"{json.dumps(api_data_context, indent=2, ensure_ascii=False)}\n"
            "--- Fin del contexto ---"
        )
        message_parts_for_llm.append({"text": context_info_text})
        logger.debug(f"LLM_SERVICE: A帽adiendo contexto de API al mensaje para user_id {user_id}: {context_info_text[:200]}...")

    try:
        logger.debug(f"LLM_SERVICE: Enviando mensaje a Gemini para user_id {user_id}. Mensaje (inicio): {str(message_parts_for_llm)[:300]}...")
        
        # Env铆a el mensaje al LLM usando la sesi贸n de chat activa.
        # `send_message_async` es preferible en un entorno asyncio.
        llm_api_response = await chat_session.send_message_async(message_parts_for_llm)
        
        # Extrae el texto de la respuesta.
        # El SDK puede tener protecciones o feedback en `response.prompt_feedback`.
        if llm_api_response.prompt_feedback and llm_api_response.prompt_feedback.block_reason:
            block_reason = llm_api_response.prompt_feedback.block_reason
            block_message = f"Respuesta bloqueada por pol铆tica de seguridad: {block_reason}."
            logger.warning(f"LLM_SERVICE: Respuesta de Gemini bloqueada para user_id {user_id}. Raz贸n: {block_reason}. Feedback: {llm_api_response.prompt_feedback}")
            # Considera qu茅 mensaje enviar al usuario en este caso.
            return f"No pude generar una respuesta completa debido a una restricci贸n de contenido ({block_reason}). Por favor, reformula tu pregunta."

        generated_text = llm_api_response.text
        logger.info(f"LLM_SERVICE: Respuesta recibida de Gemini para user_id {user_id}: {generated_text[:300]}...")

        # El historial en `chat_session.history` es actualizado autom谩ticamente por el SDK.
        # No es necesario truncar manualmente aqu铆 a menos que tengas requisitos muy espec铆ficos
        # no cubiertos por la gesti贸n de historial del SDK o si el historial crece demasiado.
        # logger.debug(f"LLM_SERVICE: Historial de chat para user_id {user_id} ahora tiene {len(chat_session.history)} turnos.")

        return generated_text

    except Exception as e:
        logger.error(f"LLM_SERVICE: Error al generar respuesta con Gemini para user_id {user_id}: {e}", exc_info=True)
        # Intenta obtener m谩s detalles del error si es espec铆fico de la API de Gemini
        if hasattr(e, 'response') and hasattr(e.response, 'prompt_feedback'):
            logger.error(f"LLM_SERVICE: Prompt Feedback de Gemini en error: {e.response.prompt_feedback}")
        # Considera si reintentar o devolver un mensaje de error gen茅rico.
        return "Lo siento, tuve un problema t茅cnico al intentar procesar tu solicitud con el asistente inteligente. Por favor, intenta de nuevo en unos momentos."


async def classify_intent_and_extract_entities(user_message: str) -> dict:
    """
    (Funci贸n Avanzada Opcional) Utiliza el LLM para una tarea espec铆fica de clasificaci贸n de intenci贸n
    y extracci贸n de entidades. Esto se ejecuta fuera del historial de chat principal.
    """
    if not llm_model:
        logger.error("LLM_SERVICE: Modelo Gemini no disponible para clasificaci贸n de intenci贸n.")
        return {"error": "Modelo Gemini no disponible."}

    # Prompt espec铆fico para la tarea de clasificaci贸n y extracci贸n.
    # Es crucial que este prompt sea claro y d茅 ejemplos del formato JSON esperado.
    classification_prompt_text = f"""
    Analiza el siguiente mensaje de usuario y determina su intenci贸n principal y extrae entidades relevantes.
    Intenciones posibles: REPORTAR_ACCIDENTE, CONSULTAR_ACCIDENTE, GUIA_VITAL, SALUDO, DESPEDIDA, PREGUNTA_GENERAL, AFIRMACION, NEGACION, CANCELAR_ACCION, DESCONOCIDO.
    
    Para REPORTAR_ACCIDENTE, extrae si es posible: "descripcion" (string), "ubicacion" (string), "fecha" (string), "hora" (string).
    Para CONSULTAR_ACCIDENTE, extrae si es posible: "ubicacion" (string), "fecha" (string), "tipo_accidente" (string), "gravedad" (string).
    
    Mensaje del usuario: "{user_message}"

    Responde NICAMENTE en formato JSON v谩lido con las claves "intent" (string) y "entities" (objeto JSON).
    Si no se extraen entidades, "entities" debe ser un objeto vac铆o {{}}.
    Si la intenci贸n no es clara o no encaja en las categor铆as, usa "DESCONOCIDO".

    Ejemplos de respuesta JSON esperada:
    Si el usuario dice "quiero reportar un choque feo en la calle 72 con 50 anoche":
    {{
      "intent": "REPORTAR_ACCIDENTE",
      "entities": {{
        "descripcion": "choque feo",
        "ubicacion": "calle 72 con 50",
        "fecha": "anoche"
      }}
    }}

    Si el usuario dice "hola":
    {{
      "intent": "SALUDO",
      "entities": {{}}
    }}
    
    Si el usuario dice "hubo accidentes hoy en la circunvalar?":
    {{
      "intent": "CONSULTAR_ACCIDENTE",
      "entities": {{
        "fecha": "hoy",
        "ubicacion": "la circunvalar"
      }}
    }}
    """
    try:
        logger.debug(f"LLM_SERVICE: Enviando prompt de clasificaci贸n/extracci贸n a Gemini (inicio): {classification_prompt_text[:300]}...")
        
        # Para tareas de una sola vez como esta, `generate_content_async` es m谩s directo. No usa historial.
        llm_classification_response = await llm_model.generate_content_async(classification_prompt_text)
        
        # Verificar si la respuesta fue bloqueada.
        if llm_classification_response.prompt_feedback and llm_classification_response.prompt_feedback.block_reason:
            block_reason = llm_classification_response.prompt_feedback.block_reason
            logger.warning(f"LLM_SERVICE: Respuesta de clasificaci贸n de Gemini bloqueada. Raz贸n: {block_reason}.")
            return {"intent": "DESCONOCIDO", "entities": {}, "error": f"Respuesta de clasificaci贸n bloqueada ({block_reason})", "raw_response": ""}

        raw_json_text = llm_classification_response.text
        logger.info(f"LLM_SERVICE: Respuesta de clasificaci贸n (raw JSON) de Gemini: {raw_json_text}")

        # Limpiar y parsear el JSON de la respuesta.
        # El LLM a veces envuelve el JSON en bloques de c贸digo (```json ... ```).
        clean_json_text = raw_json_text.strip()
        if clean_json_text.startswith("```json"):
            clean_json_text = clean_json_text[7:-3].strip() # Quita ```json\n y \n```
        elif clean_json_text.startswith("```"):
             clean_json_text = clean_json_text[3:-3].strip() # Quita ```\n y \n```
        
        parsed_response = json.loads(clean_json_text)
        
        # Validar la estructura b谩sica del JSON esperado.
        if not isinstance(parsed_response, dict) or \
           "intent" not in parsed_response or \
           not isinstance(parsed_response.get("intent"), str) or \
           "entities" not in parsed_response or \
           not isinstance(parsed_response.get("entities"), dict):
            raise ValueError("El JSON de respuesta del LLM para clasificaci贸n no tiene la estructura esperada (intent/entities).")
            
        logger.info(f"LLM_SERVICE: Intenci贸n y entidades extra铆das: {parsed_response}")
        return parsed_response

    except json.JSONDecodeError as json_err:
        logger.error(f"LLM_SERVICE: Error al parsear JSON de Gemini para clasificaci贸n: {json_err}. Respuesta original: {raw_json_text}", exc_info=True)
        return {"intent": "DESCONOCIDO", "entities": {}, "error": "Error al parsear la estructura de la respuesta del LLM.", "raw_response": raw_json_text}
    except Exception as e:
        logger.error(f"LLM_SERVICE: Error en clasificaci贸n de intenci贸n con Gemini: {e}", exc_info=True)
        if hasattr(e, 'response') and hasattr(e.response, 'prompt_feedback'): # Error espec铆fico de la API de Gemini
            logger.error(f"LLM_SERVICE: Prompt Feedback de Gemini (clasificaci贸n) en error: {e.response.prompt_feedback}")
        return {"intent": "DESCONOCIDO", "entities": {}, "error": f"Error inesperado durante la clasificaci贸n: {type(e).__name__}"}


async def reset_conversation_history(user_id: str) -> bool:
    """
    Limpia/resetea el historial de conversaci贸n (sesi贸n de chat) para un usuario espec铆fico.
    """
    if user_id in conversation_sessions_store:
        del conversation_sessions_store[user_id] # Elimina la sesi贸n de chat del almac茅n.
        logger.info(f"LLM_SERVICE: Historial de conversaci贸n (sesi贸n de chat de Gemini) reseteado para el usuario {user_id}.")
        return True
    logger.info(f"LLM_SERVICE: No se encontr贸 historial para resetear para el usuario {user_id}.")
    return False

